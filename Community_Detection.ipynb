{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "import pickle\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NLTK Downloads**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructing the Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = Path('data/twint')\n",
    "tweets_file = data_folder / 'data_new2.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(tweets_file, 'r')\n",
    "lines = [l for l in f]\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = [json.loads(l) for l in lines]\n",
    "print('Number of tweets read in:\\t%s' % len(tweets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Community Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Filtering and Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "tokenizer = TweetTokenizer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_tokenize(tweet):\n",
    "    return tokenizer.tokenize(tweet)\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "# https://stackoverflow.com/a/49146722/330558\n",
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)\n",
    "\n",
    "def preprocess_tweet_text(tweet):\n",
    "    tweet = tweet.lower()\n",
    "    #Remove numbers\n",
    "    tweet = re.sub(r\"[0-9]\", '', tweet, flags=re.MULTILINE)\n",
    "    # Remove urls\n",
    "    tweet = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', tweet, flags=re.MULTILINE)\n",
    "    # Remove punctuation\n",
    "    tweet = tweet.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Remove emojis\n",
    "    tweet = remove_emoji(tweet)\n",
    "    # Tokenize\n",
    "    tweet_tokens = word_tokenize(tweet)\n",
    "    # Remove stopwords\n",
    "    filtered_words = [w for w in tweet_tokens if not w in stop_words]\n",
    "    # Stemming\n",
    "    #shortened = [stemmer.stem(w) for w in filtered_words]\n",
    "    # Lemmatizing\n",
    "    lemmatized = [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in filtered_words]\n",
    "    # Remove non-alpha words\n",
    "    lemmatized_filtered = [w for w in lemmatized if w.isalpha()]\n",
    "    # Filter out short lemmas\n",
    "    final_tokens = [w for w in lemmatized_filtered if len(w) > 2]\n",
    "    \n",
    "    return final_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_usernames = {t['username'] for t in tweets}\n",
    "users_to_documents = {u:[] for u in unique_usernames}\n",
    "\n",
    "for tw in tweets:\n",
    "    u = tw['username']\n",
    "    t = preprocess_tweet_text(tw['tweet'])\n",
    "    users_to_documents[u] = users_to_documents[u] + t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(users_to_documents.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Length Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = {u:len(doc) for (u, doc) in users_to_documents.items()}\n",
    "min = np.min(list(lengths.values()))\n",
    "max = np.max(list(lengths.values()))\n",
    "bins = np.linspace(start=min, stop=max, num=300, endpoint=True)\n",
    "\n",
    "labels = list(range(299))\n",
    "\n",
    "lengths_df = pd.DataFrame({'Doc': list(lengths.keys()), 'Document Length': list(lengths.values())})\n",
    "\n",
    "ax = lengths_df.hist(figsize=(20, 4), grid=False, bins=len(bins), range=(30, 200), rwidth=0.8)\n",
    "ax[0][0].set_title('Document Length Distribution', fontsize=20)\n",
    "ax[0][0].set_xlabel('Document Length', fontsize=20)\n",
    "ax[0][0].set_ylabel('Number of Documents', fontsize=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.savefig('document_length_distribution.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter out any users that now have less than 120 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(users_to_documents.values()))\n",
    "users_to_documents_doc_length_filtered = {u:doc for (u, doc) in users_to_documents.items() if len(doc) >= 120}\n",
    "print(len(users_to_documents_doc_length_filtered.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexical diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexical_diversity_percentage(tokens):\n",
    "    return len(set(tokens)) / len(tokens) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexical_diversities = {u:lexical_diversity_percentage(doc) for (u, doc) in users_to_documents_doc_length_filtered.items()}\n",
    "lexical_div_df = pd.DataFrame({'Doc': list(lexical_diversities.keys()), 'Lexical Diversity': list(lexical_diversities.values())})\n",
    "ax = lexical_div_df.hist(grid=False, bins=50, figsize=(20, 10), rwidth=0.8)\n",
    "ax[0][0].set_title('Lexical Diversity Distribution', fontsize=20)\n",
    "ax[0][0].set_xlabel('Lexical Diversity', fontsize=20)\n",
    "ax[0][0].set_ylabel('Number of Documents', fontsize=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.savefig('lexical_diversity_distribution.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter out any users with a very low lexical diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(users_to_documents_doc_length_filtered))\n",
    "users_to_documents_filtered = {u:doc for (u, doc) in users_to_documents_doc_length_filtered.items() if lexical_diversity_percentage(doc) >= 48}\n",
    "len(users_to_documents_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construction of the Tf-Idf Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = users_to_documents_filtered.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_nothing(tokens):\n",
    "    return tokens\n",
    "\n",
    "vectorizer = TfidfVectorizer(lowercase=False, tokenizer=do_nothing, min_df=0.2, max_df=0.8)\n",
    "\n",
    "X = vectorizer.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_df.iloc[:10, 60:70]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_file = open('tfidf.pickle', 'wb')\n",
    "pickle.dump(tf_idf_df, tfidf_file)\n",
    "tfidf_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Load the tf-idf Matrix from Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_file = open('tfidf.pickle', 'rb')\n",
    "tf_idf_df = pickle.load(tfidf_file)\n",
    "tfidf_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform the PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "data_scaled = pd.DataFrame(preprocessing.normalize(tf_idf_df), columns=tf_idf_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=30)\n",
    "X_reduced = pca.fit_transform(data_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze the Percentage of explained Variance per Principal Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(data={'PCA': [i+1 for i in range(0, len(pca.explained_variance_ratio_))],'Explained Variance': pca.explained_variance_ratio_})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explained_variance = pca.explained_variance_ratio_.sum()\n",
    "print(\"Explained variance of the PCA step: {}%\".format(int(explained_variance * 100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "ax = plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.title('Explained Variance in Relation to the Number of Principal Components', fontsize=20)\n",
    "plt.xlabel('Explained Variance in Percentage', fontsize=20)\n",
    "plt.ylabel('Number of Principal Components', fontsize=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.savefig('explained_variance.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1, figsize=(14, 14))\n",
    "plt.scatter(X_reduced[:, 0], X_reduced[:, 1], color='lightblue', edgecolor='black', s=40)\n",
    "plt.title('Reduced Data Set', fontsize=20)\n",
    "plt.xlabel('PC 1', fontsize=20)\n",
    "plt.ylabel('PC 2', fontsize=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.savefig('pca_first_2_pcs.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3D Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(1, figsize=(14, 14))\n",
    "fig.subplots_adjust(left=0, right=1, bottom=0, top=1)\n",
    "ax = Axes3D(fig, elev=-150, azim=110)\n",
    "ax.scatter(X_reduced[:, 0], X_reduced[:, 1], X_reduced[:, 2], color='lightblue', edgecolor='black', s=40)\n",
    "ax.set_title('Reduced Data Set', fontsize=20)\n",
    "ax.set_xlabel('PC 1', fontsize=20, labelpad=40)\n",
    "ax.set_ylabel('PC 2', fontsize=20, labelpad=40)\n",
    "ax.set_zlabel('PC 3', fontsize=20, labelpad=40)\n",
    "ax.tick_params(labelsize=20, pad=20)\n",
    "plt.savefig('pca_first_3_pcs.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of components\n",
    "n_pcs= pca.components_.shape[0]\n",
    "\n",
    "# get the index of the most important feature on EACH component\n",
    "# LIST COMPREHENSION HERE\n",
    "most_important_feature_indexes_per_pc = {}\n",
    "\n",
    "initial_feature_names = data_scaled.columns\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for pc in range(n_pcs):\n",
    "    components_enumerated = list(enumerate(pca.components_[pc]))\n",
    "    components_sorted = sorted(components_enumerated, key=lambda x:np.abs(x[1]), reverse=True)\n",
    "    features_imps = {initial_feature_names[i]:imp for (i, imp) in components_sorted}\n",
    "    features_imps_df = pd.DataFrame.from_dict(features_imps, orient='index', columns = ['importance'] )[:10]\n",
    "    dfs.append(features_imps_df)\n",
    "    \n",
    "print(dfs[0])\n",
    "print()\n",
    "print(dfs[1])\n",
    "print()\n",
    "print(dfs[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perfoming the Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters=3)\n",
    "km.fit(X_reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_kmeans = km.labels_\n",
    "plt.figure(figsize=(14, 14))\n",
    "plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=y_kmeans, s=40, edgecolor='black')\n",
    "centers = km.cluster_centers_\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='red', s=400, alpha=0.8, edgecolor='black')\n",
    "plt.tick_params(labelsize=20)\n",
    "plt.xlabel('PC 1', fontsize=20)\n",
    "plt.ylabel('PC 2', fontsize=20)\n",
    "plt.title('K-Means Result', fontsize=20)\n",
    "plt.savefig('k_means_result_2d.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3D Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(1, figsize=(14, 14))\n",
    "fig.subplots_adjust(left=0, right=1, bottom=0, top=1)\n",
    "ax = Axes3D(fig, elev=-150, azim=110)\n",
    "y = range(0, len(X_reduced))\n",
    "ax.scatter(X_reduced[:, 0], X_reduced[:, 1], X_reduced[:, 2], c=y_kmeans, edgecolor='k', s=50)\n",
    "ax.scatter(centers[:, 0], centers[:, 1], centers[:, 2], c='red', s=400, alpha=0.8, edgecolor='black')\n",
    "ax.set_title('K-Means Result', fontsize=20)\n",
    "\n",
    "ax.set_xlabel('PC 1', fontsize=20, labelpad=40)\n",
    "ax.set_ylabel('PC 2', fontsize=20, labelpad=40)\n",
    "ax.set_zlabel('PC 3', fontsize=20, labelpad=40)\n",
    "ax.tick_params(labelsize=20, pad=20)\n",
    "plt.savefig('k_means_result_3d.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the Users per Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df = pd.DataFrame({'Username': list(users_to_documents_filtered.keys()), 'Cluster Label': km.labels_})\n",
    "\n",
    "for i in range(0, len(users_df)):\n",
    "    p = X_reduced[i]\n",
    "    cluster = users_df.loc[i]['Cluster Label']\n",
    "    \n",
    "users_cluster_0 = users_df[users_df['Cluster Label'] == 0]\n",
    "users_cluster_1 = users_df[users_df['Cluster Label'] == 1]\n",
    "users_cluster_2 = users_df[users_df['Cluster Label'] == 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retweets per individual user for each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retweet_count_per_users_cluster_0 = {u:0 for u in list(users_cluster_0['Username'])}\n",
    "retweet_count_per_users_cluster_1 = {u:0 for u in list(users_cluster_1['Username'])}\n",
    "retweet_count_per_users_cluster_2 = {u:0 for u in list(users_cluster_2['Username'])}\n",
    "\n",
    "for t in tweets:\n",
    "    username = t['username']\n",
    "    retweets = t['retweets_count']\n",
    "    if username in retweet_count_per_users_cluster_0:\n",
    "        retweet_count_per_users_cluster_0[username] += retweets\n",
    "    if username in retweet_count_per_users_cluster_1:\n",
    "        retweet_count_per_users_cluster_1[username] += retweets\n",
    "    if username in retweet_count_per_users_cluster_2:\n",
    "        retweet_count_per_users_cluster_2[username] += retweets\n",
    "        \n",
    "ten_most_retweeted_cluster_0 = pd.DataFrame({'Username': list(retweet_count_per_users_cluster_0.keys()), 'Retweets': list(retweet_count_per_users_cluster_0.values())}).sort_values(by='Retweets')[::-1].head(15)\n",
    "ten_most_retweeted_cluster_1 = pd.DataFrame({'Username': list(retweet_count_per_users_cluster_1.keys()), 'Retweets': list(retweet_count_per_users_cluster_1.values())}).sort_values(by='Retweets')[::-1].head(15)\n",
    "ten_most_retweeted_cluster_2 = pd.DataFrame({'Username': list(retweet_count_per_users_cluster_2.keys()), 'Retweets': list(retweet_count_per_users_cluster_2.values())}).sort_values(by='Retweets')[::-1].head(15)\n",
    "\n",
    "\n",
    "tweets_cluster_0 = []\n",
    "tweets_cluster_1 = []\n",
    "tweets_cluster_2 = []\n",
    "\n",
    "for t in tweets:\n",
    "    username = t['username']\n",
    "    tweet = t['tweet']\n",
    "    if username in list(ten_most_retweeted_cluster_0['Username']):\n",
    "        tweets_cluster_0.append(t)\n",
    "    if username in list(ten_most_retweeted_cluster_1['Username']):\n",
    "        tweets_cluster_1.append(t)\n",
    "    if username in list(ten_most_retweeted_cluster_2['Username']):\n",
    "        tweets_cluster_2.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ten_most_retweeted_cluster_0)\n",
    "print()\n",
    "print(ten_most_retweeted_cluster_1)\n",
    "print()\n",
    "print(ten_most_retweeted_cluster_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VADER Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "sentiments_cluster_0_df = pd.DataFrame(columns=['Link', 'Sentence', 'Pos', 'Neu', 'Neg', 'Compound'])\n",
    "sentiments_cluster_1_df = pd.DataFrame(columns=['Link', 'Sentence', 'Pos', 'Neu', 'Neg', 'Compound'])\n",
    "sentiments_cluster_2_df = pd.DataFrame(columns=['Link', 'Sentence', 'Pos', 'Neu', 'Neg', 'Compound'])\n",
    "\n",
    "\n",
    "for t in tweets_cluster_0:\n",
    "    ss = sid.polarity_scores(t['tweet'])\n",
    "    sentiments_cluster_0_df = sentiments_cluster_0_df.append({'Link': t['link'],\n",
    "                                                              'Sentence': t['tweet'],\n",
    "                                                             'Pos': ss['pos'],\n",
    "                                                             'Neu': ss['neu'],\n",
    "                                                             'Neg': ss['neg'],\n",
    "                                                             'Compound': ss['compound']}, ignore_index=True)\n",
    "    \n",
    "for t in tweets_cluster_1:\n",
    "    ss = sid.polarity_scores(t['tweet'])\n",
    "    sentiments_cluster_1_df = sentiments_cluster_1_df.append({'Link': t['link'],\n",
    "                                                              'Sentence': t['tweet'],\n",
    "                                                             'Pos': ss['pos'],\n",
    "                                                             'Neu': ss['neu'],\n",
    "                                                             'Neg': ss['neg'],\n",
    "                                                             'Compound': ss['compound']}, ignore_index=True)\n",
    "\n",
    "for t in tweets_cluster_2:\n",
    "    ss = sid.polarity_scores(t['tweet'])\n",
    "    sentiments_cluster_2_df = sentiments_cluster_2_df.append({'Link': t['link'],\n",
    "                                                              'Sentence': t['tweet'],\n",
    "                                                             'Pos': ss['pos'],\n",
    "                                                             'Neu': ss['neu'],\n",
    "                                                             'Neg': ss['neg'],\n",
    "                                                             'Compound': ss['compound']}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments_cluster_2_df.sort_values(by='Compound')[::-1].head(10).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the tweets per cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_cluster0 = [t for t in tweets if t['username'] in list(users_cluster_0['Username'])]    \n",
    "tweets_cluster1 = [t for t in tweets if t['username'] in list(users_cluster_1['Username'])]\n",
    "tweets_cluster2 = [t for t in tweets if t['username'] in list(users_cluster_2['Username'])]\n",
    "texts_cluster0 = [t['tweet'] for t in tweets_cluster0]\n",
    "texts_cluster1 = [t['tweet'] for t in tweets_cluster1]\n",
    "texts_cluster2 = [t['tweet'] for t in tweets_cluster2]\n",
    "print(len(texts_cluster0))\n",
    "print(len(texts_cluster1))\n",
    "print(len(texts_cluster2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most used hashtags in the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "hashtags_cluster0 = [h for t in tweets_cluster0 for h in t['hashtags']]\n",
    "hashtags_cluster1 = [h for t in tweets_cluster1 for h in t['hashtags']]\n",
    "hashtags_cluster2 = [h for t in tweets_cluster2 for h in t['hashtags']]\n",
    "\n",
    "\n",
    "c0 = Counter(hashtags_cluster0)\n",
    "c1 = Counter(hashtags_cluster1)\n",
    "c2 = Counter(hashtags_cluster2)\n",
    "\n",
    "df0 = pd.DataFrame(index=[e[0] for e in c0.most_common(10)], data={'count': [e[1] for e in c0.most_common(10)]}).sort_values(by='count')[::-1]\n",
    "df1 = pd.DataFrame(index=[e[0] for e in c1.most_common(10)], data={'count': [e[1] for e in c1.most_common(10)]}).sort_values(by='count')[::-1]\n",
    "df2 = pd.DataFrame(index=[e[0] for e in c2.most_common(10)], data={'count': [e[1] for e in c2.most_common(10)]}).sort_values(by='count')[::-1]\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(22, 13))\n",
    "\n",
    "plt.subplots_adjust(hspace=2)\n",
    "\n",
    "df0.plot(ax=axes[0], kind='bar', title='Most used Hashtags in Cluster 1', legend=False, fontsize=20)\n",
    "df1.plot(ax=axes[1], kind='bar', title='Most used Hashtags in Cluster 2', legend=False, fontsize=20)\n",
    "df2.plot(ax=axes[2], kind='bar', title='Most used Hashtags in Cluster 3', legend=False, fontsize=20)\n",
    "\n",
    "for i in range(3):\n",
    "    axes[i].title.set_size(20)\n",
    "    axes[i].tick_params(axis='x', labelrotation=45)\n",
    "\n",
    "plt.savefig('most_used_hashtags_per_cluster.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of Tweets per day per cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = [tweets_cluster0, tweets_cluster1, tweets_cluster2]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig, axes = plt.subplots(3,1, figsize=(20, 13))\n",
    "plt.subplots_adjust(hspace=0.6)\n",
    "i = 0\n",
    "for c in clusters:\n",
    "    dates_to_n_tweets = {}\n",
    "    for t in c:\n",
    "        date = t['date'].split('-')[1] + '-' + t['date'].split('-')[2]\n",
    "        try:\n",
    "            dates_to_n_tweets[date] = dates_to_n_tweets[date] + 1\n",
    "        except:\n",
    "            dates_to_n_tweets[date] = 0\n",
    "\n",
    "    sorted_dates = sorted(dates_to_n_tweets.items())\n",
    "    df = pd.DataFrame({'date': [e[0] for e in sorted_dates], 'count': [e[1] for e in sorted_dates]})\n",
    "    ax = df.plot(ax=axes[i], kind='line', title='Number of Tweets per Day in Cluster %d' % (i+1), legend=False, fontsize=20)\n",
    "    xlabels = df['date']\n",
    "    ax.set_xticklabels(xlabels, fontsize=14, rotation=90)\n",
    "    ax.set_xticks(range(len(xlabels)))\n",
    "    ax.axvline(x=37, color='red', alpha=0.5)\n",
    "    ax.axvline(x=39, color='red', alpha=0.5)\n",
    "    i+=1\n",
    "    \n",
    "axes[0].title.set_size(20)\n",
    "axes[1].title.set_size(20)\n",
    "axes[2].title.set_size(20)\n",
    "\n",
    "plt.savefig('tweets_per_day_per_cluster.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Four most liked tweets per cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_cluster0_sorted = sorted(tweets_cluster0, key=lambda t: t['likes_count'], reverse=True)\n",
    "tweets_cluster1_sorted = sorted(tweets_cluster1, key=lambda t: t['retweets_count'], reverse=True)\n",
    "tweets_cluster2_sorted = sorted(tweets_cluster2, key=lambda t: t['retweets_count'], reverse=True)\n",
    "\n",
    "#print(*[t for t in tweets_cluster0_sorted[:4]], sep='\\n_________________________________________\\n\\n\\n')\n",
    "#print('::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::')\n",
    "#print(*[t for t in tweets_cluster1_sorted[:4]], sep='\\n_________________________________________\\n\\n\\n')\n",
    "#print('::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::')\n",
    "print(*[t for t in tweets_cluster2_sorted[:4]], sep='\\n_________________________________________\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shifterator Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shifterator as sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_cluster0 = [t for (u, doc) in users_to_documents_doc_length_filtered.items() if u in list(users_cluster_0['Username']) for t in doc]    \n",
    "tokens_cluster1 = [t for (u, doc) in users_to_documents_doc_length_filtered.items() if u in list(users_cluster_1['Username']) for t in doc]    \n",
    "tokens_cluster2 = [t for (u, doc) in users_to_documents_doc_length_filtered.items() if u in list(users_cluster_2['Username']) for t in doc]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_freqs_cluster_0 = dict(Counter(tokens_cluster0))\n",
    "token_freqs_cluster_1 = dict(Counter(tokens_cluster1))\n",
    "token_freqs_cluster_2 = dict(Counter(tokens_cluster2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shift = sh.EntropyShift(type2freq_1=token_freqs_cluster_0, type2freq_2=token_freqs_cluster_1)\n",
    "shift.get_shift_graph(system_names = ['Cluster 1', 'Cluster 2'], filename='entropy_shift_cluster1_cluster2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python38264bit4df60f5f9505484d8ff9ba651e850bad"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
